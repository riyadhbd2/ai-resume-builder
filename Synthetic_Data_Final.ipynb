{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs6rqTRi0atV0eeJv7633X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyadhbd2/ai-resume-builder/blob/main/Synthetic_Data_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-fGP7ImfV76",
        "outputId": "80a54160-4ce1-4e05-8283-e012b007712d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Faker\n",
            "  Downloading Faker-26.0.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->Faker) (1.16.0)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-26.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Faker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DF\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "\n",
        "# Set up Faker instance\n",
        "fake = Faker()\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Original data\n",
        "original_data = [\n",
        "   {\"self_test\": \"English proficiency test\", \"item_name\": \"Laptop\", \"cgpa\": 3.5},\n",
        "   # Add more data points as needed\n",
        "]\n",
        "\n",
        "# Function to generate synthetic data\n",
        "def generate_synthetic_data(original_data, num_course=1, num_student=500):\n",
        "   synthetic_data = []\n",
        "   uid_dict = {}  # Dictionary to store UID for each Student_ID\n",
        "   for j in range(1, num_student + 1):\n",
        "       # Generate matriculation number outside inner loop for unique student ID\n",
        "       matriculation_number = np.random.randint(100000, 999999)\n",
        "\n",
        "       # Generate synthetic first name in SN1, SN2, SN3 format\n",
        "       first_name = \"SN\" + str(j)\n",
        "\n",
        "       # Generate synthetic last name in LN1, LN2, LN3 format\n",
        "       last_name = \"LN\" + str(j)\n",
        "\n",
        "       # Generate email in the format \"SN1-LN1@tuchemnitz.de\"\n",
        "       email = f\"{last_name}-{first_name}@tuchemnitz.de\"\n",
        "\n",
        "       # Generate synthetic student user ID only once per Student_ID\n",
        "       if j not in uid_dict:\n",
        "           uid_dict[j] = ''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'), size=30))\n",
        "       uid = uid_dict[j]\n",
        "\n",
        "       for i in range(num_course):\n",
        "           # Create a synthetic data point\n",
        "           synthetic_data.append({\n",
        "               \"Student_ID\": j,\n",
        "               \"Matriculation_Number\": matriculation_number,\n",
        "               \"Last_Name\": last_name,\n",
        "               \"First_Name\": first_name,\n",
        "               \"Email\": email,\n",
        "               \"UID\": uid\n",
        "           })\n",
        "   return synthetic_data\n",
        "\n",
        "# Create DataFrame df\n",
        "synthetic_data = generate_synthetic_data(original_data)\n",
        "df = pd.DataFrame(synthetic_data)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktNqPy6DfbOq",
        "outputId": "dcdfcf9c-333b-4cb9-99cc-374255890081"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Student_ID  Matriculation_Number Last_Name First_Name  \\\n",
            "0             1                221958       LN1        SN1   \n",
            "1             2                284779       LN2        SN2   \n",
            "2             3                765987       LN3        SN3   \n",
            "3             4                415139       LN4        SN4   \n",
            "4             5                532315       LN5        SN5   \n",
            "..          ...                   ...       ...        ...   \n",
            "495         496                149810     LN496      SN496   \n",
            "496         497                465213     LN497      SN497   \n",
            "497         498                782527     LN498      SN498   \n",
            "498         499                327997     LN499      SN499   \n",
            "499         500                394849     LN500      SN500   \n",
            "\n",
            "                         Email                             UID  \n",
            "0        LN1-SN1@tuchemnitz.de  ZCoQh8uM5swkkx0JNxcv0bxRDLb7uG  \n",
            "1        LN2-SN2@tuchemnitz.de  5v8RyWA6PB7po99U9YR2Z4cKYguiMr  \n",
            "2        LN3-SN3@tuchemnitz.de  y7nX5iz0btBU7gR8hUInqJXNdb9f1P  \n",
            "3        LN4-SN4@tuchemnitz.de  1CrzRHj9JnEVohnw749NupSrU0xzy7  \n",
            "4        LN5-SN5@tuchemnitz.de  7SOCoSaygixaRhxkYqhIIG6ePM5OBg  \n",
            "..                         ...                             ...  \n",
            "495  LN496-SN496@tuchemnitz.de  gzoU3LcLpbEBgDQ59gJCxZMdCRe2C9  \n",
            "496  LN497-SN497@tuchemnitz.de  CzszKADY0Dc65Qg8W4i0OPmo1eCIAA  \n",
            "497  LN498-SN498@tuchemnitz.de  tjmCI81hCEODvulkbXObHcpYdw4Xty  \n",
            "498  LN499-SN499@tuchemnitz.de  rPkCqz34w3ZVtSbxEAb4ndWmJIR12h  \n",
            "499  LN500-SN500@tuchemnitz.de  fE1jRuPjwcFtjCOZYTHKt88hgfkRrY  \n",
            "\n",
            "[500 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DF2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Extract the number of unique students from df\n",
        "num_student = df['Student_ID'].nunique()\n",
        "\n",
        "# Constants\n",
        "num_topics = 39\n",
        "num_semesters = 10\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize the list for df2 data\n",
        "df2_data = []\n",
        "\n",
        "# Dictionary to keep track of student count for each (topic_id, semester_id) pair\n",
        "student_count = defaultdict(set)\n",
        "\n",
        "# Counter to keep track of the number of unique (Student_ID, Topic_Id, Semester_Id) rows\n",
        "total_rows_needed = num_student\n",
        "\n",
        "while len(df2_data) < total_rows_needed:\n",
        "    student_id = np.random.randint(1, num_student + 1)\n",
        "    topic_id = np.random.randint(1, num_topics + 1)\n",
        "    semester_id = np.random.randint(1, num_semesters + 1)\n",
        "\n",
        "    # Check if the student count for this (topic_id, semester_id) pair is less than 4\n",
        "    if len(student_count[(topic_id, semester_id)]) < 4:\n",
        "        if student_id not in student_count[(topic_id, semester_id)]:\n",
        "            df2_data.append({\n",
        "                \"Student_Id\": student_id,\n",
        "                \"Topic_Id\": topic_id,\n",
        "                \"Semester_Id\": semester_id\n",
        "            })\n",
        "            student_count[(topic_id, semester_id)].add(student_id)\n",
        "\n",
        "# Create the DataFrame df2\n",
        "df2 = pd.DataFrame(df2_data)\n",
        "print(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ijN5uOfezc",
        "outputId": "6c4a1cdd-b0a2-49b6-eff1-5a70a45fab15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Student_Id  Topic_Id  Semester_Id\n",
            "0           103        29            8\n",
            "1           189        21            7\n",
            "2           122        19            7\n",
            "3           331        11            8\n",
            "4           373        36            8\n",
            "..          ...       ...          ...\n",
            "495         158         4            2\n",
            "496         446        37            9\n",
            "497         432         1            5\n",
            "498         494        15            9\n",
            "499         399        24            4\n",
            "\n",
            "[500 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DF3\n",
        "topic_names = [\n",
        "    \"Image segmentation based on Convolutional Neural Network\", \"Advanced Driver Assistance Systems ADAS\",\n",
        "    \"Standards for Safeguarding Highly Automated Driving\", \"Depth estimation from monocular camera based on Convolutional Neural Networks\",\n",
        "    \"Car2X Communication Protocol\", \"Technologies for Internet of Things\", \"AUTOSAR Application Development\",\n",
        "    \"AUTOSAR RTE Test\", \"Image Classification based on Convolutional Neural Network\", \"SLAM with Stereo Cameras\",\n",
        "    \"Car2X Communication Virtual Sensors\", \"Automotive Communication Buses\", \"Car2X Communication Concepts Limitations\",\n",
        "    \"Cyber Physical System\", \"Automotive Ethernet\", \"Depth estimation from stereo camera based on Convolutional Neural Networks\",\n",
        "    \"FPGA based Template Matching for Object Detection in High resolution Image Data\", \"Noise Suppression\",\n",
        "    \"Mixed Criticality Scheduling for Real Time Systems\", \"FPGA based Watershed Segmentation for High resolution Image Data\",\n",
        "    \"AUTOSAR Partitioning\", \"Multicore and AUTOSAR\", \"Adaptive Cruise Control Systems\", \"Car2X Communication Protocols\",\n",
        "    \"Image segmentation based on Convolutional Neural Networks\", \"Analysis of Position Estimation Methods of MAVs\",\n",
        "    \"Comparison of Deep Learning Methods for Impact Detection\", \"Speaker Separation by Computational Auditory Scene Analysis using Directional Filter\",\n",
        "    \"Image Classification based on Convolutional Neural Networks\", \"Sound Source Separation\", \"SLAM with Monocular Cameras\",\n",
        "    \"Bluetooth LE BACHELOR ONLY\", \"Universal Serial Bus Host BACHELOR ONLY\", \"Sound Source Localization\",\n",
        "    \"Mobile detection of Smart City Data for automated Cloud System Evaluation\", \"Automated Evaluation of Smart City Data from Cloud System\",\n",
        "    \"Visualization of Smart City Data from automated Cloud System\", \"Applications of Electroencephalography Based Brain Computer Interface in the Automotive Domain\",\n",
        "    \"Object Detection in 3D point clouds using Deep Learning\"\n",
        "]\n",
        "\n",
        "topic_data = {\n",
        "    \"Topic_ID\": np.arange(1, num_topics + 1),\n",
        "    \"Topic_Name\": topic_names\n",
        "}\n",
        "topic = pd.DataFrame(topic_data)\n",
        "print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1acR0mJfkzU",
        "outputId": "7771d05b-083c-472b-c19b-be90759ef93e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Topic_ID                                         Topic_Name\n",
            "0          1  Image segmentation based on Convolutional Neur...\n",
            "1          2            Advanced Driver Assistance Systems ADAS\n",
            "2          3  Standards for Safeguarding Highly Automated Dr...\n",
            "3          4  Depth estimation from monocular camera based o...\n",
            "4          5                       Car2X Communication Protocol\n",
            "5          6                Technologies for Internet of Things\n",
            "6          7                    AUTOSAR Application Development\n",
            "7          8                                   AUTOSAR RTE Test\n",
            "8          9  Image Classification based on Convolutional Ne...\n",
            "9         10                           SLAM with Stereo Cameras\n",
            "10        11                Car2X Communication Virtual Sensors\n",
            "11        12                     Automotive Communication Buses\n",
            "12        13           Car2X Communication Concepts Limitations\n",
            "13        14                              Cyber Physical System\n",
            "14        15                                Automotive Ethernet\n",
            "15        16  Depth estimation from stereo camera based on C...\n",
            "16        17  FPGA based Template Matching for Object Detect...\n",
            "17        18                                  Noise Suppression\n",
            "18        19  Mixed Criticality Scheduling for Real Time Sys...\n",
            "19        20  FPGA based Watershed Segmentation for High res...\n",
            "20        21                               AUTOSAR Partitioning\n",
            "21        22                              Multicore and AUTOSAR\n",
            "22        23                    Adaptive Cruise Control Systems\n",
            "23        24                      Car2X Communication Protocols\n",
            "24        25  Image segmentation based on Convolutional Neur...\n",
            "25        26    Analysis of Position Estimation Methods of MAVs\n",
            "26        27  Comparison of Deep Learning Methods for Impact...\n",
            "27        28  Speaker Separation by Computational Auditory S...\n",
            "28        29  Image Classification based on Convolutional Ne...\n",
            "29        30                            Sound Source Separation\n",
            "30        31                        SLAM with Monocular Cameras\n",
            "31        32                         Bluetooth LE BACHELOR ONLY\n",
            "32        33            Universal Serial Bus Host BACHELOR ONLY\n",
            "33        34                          Sound Source Localization\n",
            "34        35  Mobile detection of Smart City Data for automa...\n",
            "35        36  Automated Evaluation of Smart City Data from C...\n",
            "36        37  Visualization of Smart City Data from automate...\n",
            "37        38  Applications of Electroencephalography Based B...\n",
            "38        39  Object Detection in 3D point clouds using Deep...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame df4\n",
        "semester_names = [\"SS_2020\", \"SS_2021\", \"SS_2022\", \"SS_2023\", \"SS_2024\", \"WS_2019\",\n",
        "                  \"WS_2020\", \"WS_2021\", \"WS_2022\", \"WS_2023\"]\n",
        "\n",
        "semester_data = {\n",
        "    \"Semester_ID\": np.arange(1, num_semesters + 1),\n",
        "    \"Semester_Name\": semester_names\n",
        "}\n",
        "semester = pd.DataFrame(semester_data)\n",
        "print(semester)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3zoYOZqfud7",
        "outputId": "09a7db7b-6908-4fed-d3cb-232dcdbbe30b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Semester_ID Semester_Name\n",
            "0            1       SS_2020\n",
            "1            2       SS_2021\n",
            "2            3       SS_2022\n",
            "3            4       SS_2023\n",
            "4            5       SS_2024\n",
            "5            6       WS_2019\n",
            "6            7       WS_2020\n",
            "7            8       WS_2021\n",
            "8            9       WS_2022\n",
            "9           10       WS_2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DF5\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# Constants\n",
        "num_semesters = 10\n",
        "\n",
        "# Test groups\n",
        "test_group1 = ['Topic Recommender', 'ARS_Search_Gr1', 'ARS_Presentation_Gr1', 'ARS_Discussion_Gr1', 'ARS_Report_Gr1', 'Self_Test_Search', 'Self_Test_Presentation', 'Self_Test_Discussion', 'Self_Test_Report']\n",
        "test_group2 = ['Topic Recommender', 'ARS_Search_Gr2', 'ARS_Presentation_Gr2', 'ARS_Discussion_Gr2', 'ARS_Report_Gr2', 'Self_Test_Search', 'Self_Test_Presentation', 'Self_Test_Discussion', 'Self_Test_Report']\n",
        "\n",
        "# Calculate number of unique students\n",
        "num_unique_students = df['Student_ID'].nunique()\n",
        "\n",
        "# Calculate number of students for each semester\n",
        "students_per_semester = num_unique_students // num_semesters\n",
        "\n",
        "# Calculate number of students for each condition\n",
        "num_students_70_percent = int(0.7 * students_per_semester)\n",
        "num_students_20_percent = int(0.2 * students_per_semester)\n",
        "num_students_10_percent = int(0.1 * students_per_semester)\n",
        "\n",
        "# Initialize lists to store row dictionaries\n",
        "rows = []\n",
        "\n",
        "# Track the students assigned to each condition per semester\n",
        "students_per_condition = {semester_id: {'70_percent': [], '20_percent': [], '10_percent': []} for semester_id in range(1, num_semesters + 1)}\n",
        "\n",
        "# Process for each semester\n",
        "for semester_id in range(1, num_semesters + 1):\n",
        "    available_students = df['Student_ID'].unique()\n",
        "\n",
        "    # Select students randomly to meet the 70% condition\n",
        "    students_meeting_70_percent = np.random.choice(available_students, size=num_students_70_percent, replace=False)\n",
        "    students_per_condition[semester_id]['70_percent'] = students_meeting_70_percent.tolist()\n",
        "\n",
        "    # Assign exactly 9 tests from test_group1 or test_group2 to each student meeting the 70% condition\n",
        "    chosen_list_70 = random.choice([test_group1, test_group2])\n",
        "    for student_id in students_meeting_70_percent:\n",
        "        tests_selected = np.random.choice(chosen_list_70, size=9, replace=False)\n",
        "        for test_name in tests_selected:\n",
        "            row_data = {\n",
        "                \"Test_ID\": len(rows) + 1,\n",
        "                \"Test_Name\": test_name,\n",
        "                \"Student_ID\": student_id,\n",
        "                \"Semester_ID\": semester_id,\n",
        "                \"Duration (in seconds)\": np.random.randint(30, 601),\n",
        "                \"Point_Reached\": int(np.random.uniform(1, 10)),  # Initial arbitrary max\n",
        "                \"Max-Point\": 17  # Initial arbitrary max\n",
        "            }\n",
        "            rows.append(row_data)\n",
        "\n",
        "    # Exclude students already selected for the 70% condition\n",
        "    available_students = [sid for sid in available_students if sid not in students_meeting_70_percent]\n",
        "\n",
        "    # Select students randomly to meet the additional 20% condition\n",
        "    students_meeting_20_percent = np.random.choice(available_students, size=num_students_20_percent, replace=False)\n",
        "    students_per_condition[semester_id]['20_percent'] = students_meeting_20_percent.tolist()\n",
        "\n",
        "    # Assign exactly 9 tests to each student meeting the additional 20% condition\n",
        "    chosen_list_20 = random.choice([test_group1, test_group2])\n",
        "    for student_id in students_meeting_20_percent:\n",
        "        tests_selected = np.random.choice(chosen_list_20, size=9, replace=False)\n",
        "\n",
        "        # Repeat 'ST' test types for this student\n",
        "        st_tests = ['Self_Test_Search', 'Self_Test_Presentation', 'Self_Test_Discussion', 'Self_Test_Report']\n",
        "        st_repeats = np.random.randint(2, 4)  # Choose between 2 or 3 repetitions\n",
        "\n",
        "        for _ in range(st_repeats):\n",
        "            for st_test_name in st_tests:\n",
        "                row_data = {\n",
        "                    \"Test_ID\": len(rows) + 1,\n",
        "                    \"Test_Name\": st_test_name,\n",
        "                    \"Student_ID\": student_id,\n",
        "                    \"Semester_ID\": semester_id,\n",
        "                    \"Duration (in seconds)\": np.random.randint(30, 601),\n",
        "                    \"Point_Reached\": int(np.random.uniform(1, 10)),  # Initial arbitrary max\n",
        "                    \"Max-Point\": 17  # Initial arbitrary max\n",
        "                }\n",
        "                rows.append(row_data)\n",
        "\n",
        "        for test_name in tests_selected:\n",
        "            row_data = {\n",
        "                \"Test_ID\": len(rows) + 1,\n",
        "                \"Test_Name\": test_name,\n",
        "                \"Student_ID\": student_id,\n",
        "                \"Semester_ID\": semester_id,\n",
        "                \"Duration (in seconds)\": np.random.randint(30, 601),\n",
        "                \"Point_Reached\": int(np.random.uniform(1, 10)),  # Initial arbitrary max\n",
        "                \"Max-Point\": 17  # Initial arbitrary max\n",
        "            }\n",
        "            rows.append(row_data)\n",
        "\n",
        "    # Exclude students already selected for the 70% and 20% conditions\n",
        "    available_students = [sid for sid in available_students if sid not in students_meeting_20_percent]\n",
        "\n",
        "    # Select students randomly to exclude from any tests (10% condition)\n",
        "    students_excluded = np.random.choice(available_students, size=num_students_10_percent, replace=False)\n",
        "    students_per_condition[semester_id]['10_percent'] = students_excluded.tolist()\n",
        "\n",
        "# Create DataFrame\n",
        "df5 = pd.DataFrame(rows)\n",
        "df5 = df5[~df5['Student_ID'].isin([sid for sem in students_per_condition.values() for sid in sem['10_percent']])]\n",
        "\n",
        "# Calculate Tries based on Test_Name repetition for each Student_ID\n",
        "df5['Tries'] = df5.groupby(['Student_ID', 'Test_Name']).cumcount() + 1\n",
        "\n",
        "# Adjust Tries to 2 or 3 for repeated tests\n",
        "df5.loc[df5['Tries'] > 3, 'Tries'] = 3\n",
        "\n",
        "# Add Test_Type column based on specified logic\n",
        "test_type = []\n",
        "for name in df5['Test_Name']:\n",
        "    if name == 'Topic Recommender':\n",
        "        test_type.append('TR')\n",
        "    elif name in ['Self_Test_Search', 'Self_Test_Presentation', 'Self_Test_Discussion', 'Self_Test_Report']:\n",
        "        test_type.append('ST')\n",
        "    else:\n",
        "        test_type.append('ARS')\n",
        "\n",
        "df5['Test_Type'] = test_type\n",
        "\n",
        "# Function to generate random date based on conditions\n",
        "def generate_date(semester_id):\n",
        "    if semester_id == 1:\n",
        "        year = 2020\n",
        "        month = random.randint(4, 9)  # April to September\n",
        "    elif semester_id == 2:\n",
        "        year = 2021\n",
        "        month = random.randint(4, 9)  # April to September\n",
        "    elif semester_id == 3:\n",
        "        year = 2022\n",
        "        month = random.randint(4, 9)  # April to September\n",
        "    elif semester_id == 4:\n",
        "        year = 2023\n",
        "        month = random.randint(4, 9)  # April to September\n",
        "    elif semester_id == 5:\n",
        "        year = 2024\n",
        "        month = random.randint(4, 9)  # April to September\n",
        "    elif semester_id == 6:\n",
        "        year = 2019\n",
        "        month = random.randint(10, 12) if random.random() < 0.5 else random.randint(1, 3)  # October to March\n",
        "    elif semester_id == 7:\n",
        "        year = 2020\n",
        "        month = random.randint(10, 12) if random.random() < 0.5 else random.randint(1, 3)  # October to March\n",
        "    elif semester_id == 8:\n",
        "        year = 2021\n",
        "        month = random.randint(10, 12) if random.random() < 0.5 else random.randint(1, 3)  # October to March\n",
        "    elif semester_id == 9:\n",
        "        year = 2022\n",
        "        month = random.randint(10, 12) if random.random() < 0.5 else random.randint(1, 3)  # October to March\n",
        "    elif semester_id == 10:\n",
        "        year = 2023\n",
        "        month = random.randint(10, 12) if random.random() < 0.5 else random.randint(1, 3)  # October to March\n",
        "\n",
        "    day = random.randint(1, 28)  # To avoid issues with different month lengths\n",
        "    hour = random.randint(0, 23)\n",
        "    minute = random.randint(0, 59)\n",
        "    second = random.randint(0, 59)\n",
        "\n",
        "    return datetime(year, month, day, hour, minute, second)\n",
        "\n",
        "# Generate Date column\n",
        "df5['Date'] = df5['Semester_ID'].apply(generate_date)\n",
        "\n",
        "# Format Date column to the required format\n",
        "df5['Date'] = df5['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Define the mappings for Max-Point based on Test_Name\n",
        "max_point_mapping = {\n",
        "    'Topic Recommender': 17,\n",
        "    'ARS_Search_Gr1': 12,\n",
        "    'ARS_Search_Gr2': 12,\n",
        "    'Self_Test_Search': 12,\n",
        "    'ARS_Presentation_Gr1': 18,\n",
        "    'ARS_Presentation_Gr2': 18,\n",
        "    'Self_Test_Presentation': 18,\n",
        "    'ARS_Discussion_Gr1': 4,\n",
        "    'ARS_Discussion_Gr2': 4,\n",
        "    'Self_Test_Discussion': 4,\n",
        "    'ARS_Report_Gr1': 3,\n",
        "    'ARS_Report_Gr2': 3,\n",
        "    'Self_Test_Report': 3\n",
        "}\n",
        "\n",
        "# Apply the Max-Point mapping to the dataframe\n",
        "df5['Max-Point'] = df5['Test_Name'].map(max_point_mapping)\n",
        "\n",
        "# Ensure Point_Reached does not exceed the new Max-Point\n",
        "df5['Point_Reached'] = df5.apply(lambda row: min(row['Point_Reached'], row['Max-Point']), axis=1)\n",
        "\n",
        "# Reorder the columns as specified\n",
        "df5 = df5[['Test_ID', 'Test_Name', 'Test_Type','Student_ID', 'Semester_ID', 'Date',  'Duration (in seconds)', 'Tries', 'Point_Reached', 'Max-Point']]\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po5HUReVfxlS",
        "outputId": "325d1bc3-3a36-489f-9f15-37cf65304911"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Test_ID               Test_Name Test_Type  Student_ID  Semester_ID  \\\n",
            "0           1  Self_Test_Presentation        ST         452            1   \n",
            "1           2       Topic Recommender        TR         452            1   \n",
            "2           3          ARS_Report_Gr1       ARS         452            1   \n",
            "3           4    ARS_Presentation_Gr1       ARS         452            1   \n",
            "4           5    Self_Test_Discussion        ST         452            1   \n",
            "...       ...                     ...       ...         ...          ...   \n",
            "5049     5050      ARS_Discussion_Gr1       ARS         282           10   \n",
            "5050     5051        Self_Test_Report        ST         282           10   \n",
            "5051     5052       Topic Recommender        TR         282           10   \n",
            "5052     5053  Self_Test_Presentation        ST         282           10   \n",
            "5053     5054    Self_Test_Discussion        ST         282           10   \n",
            "\n",
            "                     Date  Duration (in seconds)  Tries  Point_Reached  \\\n",
            "0     2020-05-04 17:32:31                    184      1              1   \n",
            "1     2020-08-17 18:49:47                    505      1              8   \n",
            "2     2020-05-27 10:49:38                     50      1              3   \n",
            "3     2020-07-26 05:36:19                    138      1              5   \n",
            "4     2020-09-15 06:04:34                    158      1              1   \n",
            "...                   ...                    ...    ...            ...   \n",
            "5049  2023-01-14 18:37:54                    242      1              4   \n",
            "5050  2023-11-24 09:01:38                    250      3              3   \n",
            "5051  2023-12-08 02:05:25                    181      1              7   \n",
            "5052  2023-03-06 16:12:00                    305      3              4   \n",
            "5053  2023-12-20 01:28:36                    251      3              4   \n",
            "\n",
            "      Max-Point  \n",
            "0            18  \n",
            "1            17  \n",
            "2             3  \n",
            "3            18  \n",
            "4             4  \n",
            "...         ...  \n",
            "5049          4  \n",
            "5050          3  \n",
            "5051         17  \n",
            "5052         18  \n",
            "5053          4  \n",
            "\n",
            "[4524 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DF6\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the 17 distinct Task_Name values\n",
        "task_names = [\"AUTOSAR\", \"Functional Safety\", \"Methodology\", \"Development Process\", \"Tools\",\n",
        "              \"Automation\", \"Smart city\", \"System Safety\", \"Artificial Intelligence (AI)\", \"Protocols\",\n",
        "              \"FPGA\", \"Image Processing\", \"Acoustics\", \"Algorithms\", \"Functionality\", \"Conceptualization\",\n",
        "              \"Complex Systems\"]\n",
        "\n",
        "# Define the 17 distinct Task_ID values\n",
        "task_ids = [\"id045f0e5d-c0ce-4832-9357-10a8a9142e18\", \"id4f02b695-711a-4426-b533-381b206b9206\",\n",
        "            \"id654aebe9-2d82-4eca-bb68-123fac8c0804\", \"id9beb064a-60c8-4e1b-b31d-062a3ee9cd28\",\n",
        "            \"idfc18f003-3d6c-4b71-a227-95c815d006ab\", \"id941d1e27-6e3c-48db-a35b-f19519e333ad\",\n",
        "            \"idd619dd22-609c-4029-9e82-f3a5e66a9ba0\", \"id75d54c62-7223-4122-b91c-9109e60e9825\",\n",
        "            \"id46625526-0c4e-4cf9-969d-9669ee360d98\", \"id858a7861-caaf-4a5c-83c4-6f88b8988e0e\",\n",
        "            \"id3dce97ba-3570-41e1-989c-15db21ff69b2\", \"id135bcf77-5c02-41c4-af81-f79c720341e5\",\n",
        "            \"ideea812e4-28f5-44bd-845a-70f769a5c9f3\", \"id97a4119b-0f74-4e7e-a486-0f0fb77b77b1\",\n",
        "            \"id5540cf88-e826-4390-9e0f-779595968c3f\", \"idc467a2d7-f835-4b32-836c-58686a01c636\",\n",
        "            \"id3de5d417-73cf-4f12-88ff-ba9f62a1380f\"]\n",
        "\n",
        "\n",
        "# Step 1: Extract rows where test_name is 'TR'\n",
        "tr_rows = df5[df5['Test_Type'] == 'TR']\n",
        "\n",
        "# Possible point values\n",
        "possible_points = [0, 1, 0.33, 0.67]\n",
        "\n",
        "def generate_random_points(total, count=17):\n",
        "    if total < 1 or total > 15:\n",
        "        raise ValueError(\"Total points must be between 1 and 15.\")\n",
        "\n",
        "    sequences = {\n",
        "        1: [0.33, 0.67] + [0] * 15,\n",
        "        2: [0.33, 0.67, 1] + [0] * 14,\n",
        "        3: [0.33, 0.67, 1, 0.33, 0.67] + [0] * 12,\n",
        "        4: [0.33, 0.67, 1, 0.33, 0.67, 1] + [0] * 11,\n",
        "        5: [0.33, 0.67, 1, 0.33, 0.67, 1, 1] + [0] * 10,\n",
        "        6: [0.33, 0.67, 1, 0.33, 0.67, 1, 1, 0.33, 0.67] + [0] * 8,\n",
        "        7: [0.33, 0.67, 1, 0.33, 0.67, 1, 1, 0.33, 0.67, 1] + [0] * 7,\n",
        "        8: [0.33, 0.67, 1, 0.33, 0.67, 1, 1, 0.33, 0.67, 1, 1] + [0] * 6,\n",
        "        9: [0.33, 0.67, 1, 0.33, 0.67, 1, 1, 0.33, 0.67, 1, 1, 1] + [0] * 5,\n",
        "        10: [0.33, 0.67, 1, 0.33, 0.67, 1, 1, 0.33, 0.67, 1, 1, 0.33,0.67, 1] + [0] * 3,\n",
        "        11: [0.33, 0.67, 1, 0.33, 0.67, 1, 0.33, 0.67, 0.33, 0.67, 1, 1, 0.33,0.67, 1, 0.33, 0.67],\n",
        "        12: [1,0.33, 0.67,0, 1, 0.33, 0.67, 1, 1, 0.33, 0.67, 1, 1, 0.33,0.67, 1,1],\n",
        "        13: [1, 1, 0.33, 0.67, 1, 0.33, 0.67, 1, 1, 1, 0.33,0.67, 1, 0.33, 0.67,1,1],\n",
        "        14: [0, 1, 0.33, 0.67, 1, 1, 1, 1, 1, 0.33,0.67, 1, 1,1,1,1,1],\n",
        "        15: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.33,0.67, 1, 1,1, 1,0]\n",
        "    }\n",
        "\n",
        "    points = sequences[total]\n",
        "    np.random.shuffle(points)\n",
        "\n",
        "    return np.array(points[:count])\n",
        "\n",
        "# Step 2: For each unique student_id, generate 17 rows with random points\n",
        "df6_list = []\n",
        "for idx, row in tr_rows.iterrows():\n",
        "    student_id = row['Student_ID']\n",
        "    test_id = row['Test_ID']\n",
        "    total_points = row['Point_Reached']\n",
        "    random_points = generate_random_points(total_points)\n",
        "\n",
        "    df6_list.append(pd.DataFrame({\n",
        "        # 'test_name': ['TR'] * 17,\n",
        "        'Student_ID': [student_id] * 17,\n",
        "        'Test_ID': test_id,\n",
        "        'Task_Name': task_names,\n",
        "        'Task_ID': task_ids,\n",
        "        'Points_Reached': random_points\n",
        "    }))\n",
        "\n",
        "# Step 3: Combine all the generated DataFrames into a single DataFrame\n",
        "df6 = pd.concat(df6_list, ignore_index=True)\n",
        "\n",
        "# Step 4: Add a serial column\n",
        "if 'index' in df2.columns:\n",
        "    df2 = df2.drop(columns=['index'])\n",
        "df6 = df6.reset_index(drop=True)\n",
        "df6.insert(0, 'serial', range(1, len(df6) + 1))\n",
        "print(df6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZZsHfksf1ZM",
        "outputId": "ff9594d9-8ff4-407b-85f8-7ba7b763cc4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      serial  Student_ID  Test_ID            Task_Name  \\\n",
            "0          1         445        4              AUTOSAR   \n",
            "1          2         445        4    Functional Safety   \n",
            "2          3         445        4          Methodology   \n",
            "3          4         445        4  Development Process   \n",
            "4          5         445        4                Tools   \n",
            "...      ...         ...      ...                  ...   \n",
            "7067    7068         109     5057            Acoustics   \n",
            "7068    7069         109     5057           Algorithms   \n",
            "7069    7070         109     5057        Functionality   \n",
            "7070    7071         109     5057    Conceptualization   \n",
            "7071    7072         109     5057      Complex Systems   \n",
            "\n",
            "                                     Task_ID  Points_Reached  \n",
            "0     id045f0e5d-c0ce-4832-9357-10a8a9142e18            0.67  \n",
            "1     id4f02b695-711a-4426-b533-381b206b9206            1.00  \n",
            "2     id654aebe9-2d82-4eca-bb68-123fac8c0804            0.33  \n",
            "3     id9beb064a-60c8-4e1b-b31d-062a3ee9cd28            0.00  \n",
            "4     idfc18f003-3d6c-4b71-a227-95c815d006ab            0.67  \n",
            "...                                      ...             ...  \n",
            "7067  ideea812e4-28f5-44bd-845a-70f769a5c9f3            0.00  \n",
            "7068  id97a4119b-0f74-4e7e-a486-0f0fb77b77b1            0.00  \n",
            "7069  id5540cf88-e826-4390-9e0f-779595968c3f            0.00  \n",
            "7070  idc467a2d7-f835-4b32-836c-58686a01c636            0.00  \n",
            "7071  id3de5d417-73cf-4f12-88ff-ba9f62a1380f            0.67  \n",
            "\n",
            "[7072 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DF7\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df5 is the existing DataFrame as previously created\n",
        "\n",
        "# Step 1: Create df7 with the required columns from df5\n",
        "df7 = df5[['Student_ID', 'Test_ID', 'Test_Name', 'Test_Type']].copy()\n",
        "\n",
        "# Step 2: Rename columns to match the requirements\n",
        "df7 = df7.rename(columns={\n",
        "    'Student_ID': 'Student_ID',\n",
        "    'Test_ID': 'Test_ID',\n",
        "    'Test_Name': 'Test_Name',\n",
        "    'Test_Type': 'Test_Type'\n",
        "})\n",
        "\n",
        "# Step 3: Filter out rows where the Test_Name is 'Topic Recommender'\n",
        "df7 = df7[df7['Test_Name'] != 'Topic Recommender']\n",
        "\n",
        "# Step 4: Define the repeat conditions, Task_Name, and Task_ID mappings\n",
        "repeat_conditions = {\n",
        "    'Presentation': {\n",
        "        'repeat_count': 2,\n",
        "        'task_names': [\"Scientific_Presentation_Style1\", \"Scientific_Presentation_Style2\"],\n",
        "        'task_ids': [\n",
        "            \"idaf224ec3-be2d-45bb-9f3c-acc818407763\",\n",
        "            \"idfce2e221-452f-4828-bd05-266c2d27e3c0\"\n",
        "        ]\n",
        "    },\n",
        "    'Discussion': {\n",
        "        'repeat_count': 4,\n",
        "        'task_names': [\n",
        "            \"Typical_Question1\",\n",
        "            \"Typical_Question2\",\n",
        "            \"Typical_Question3\",\n",
        "            \"Typical_Question4\"\n",
        "        ],\n",
        "        'task_ids': [\n",
        "            \"id072938f8-9d4c-464a-a096-7779cde74fe6\",\n",
        "            \"id6c761817-f2fe-47bd-8499-540aeed1cbcf\",\n",
        "            \"id615426a1-9c8d-487a-9a49-d8636a489b31\",\n",
        "            \"id4ee22677-08a9-4e7d-94d1-a9bf1fed9aaa\"\n",
        "        ]\n",
        "    },\n",
        "    'Report': {\n",
        "        'repeat_count': 3,\n",
        "        'task_names': [\n",
        "            \"Direct_Quotation1\",\n",
        "            \"Direct_Quotation2\",\n",
        "            \"Indirect_Quotation\"\n",
        "        ],\n",
        "        'task_ids': [\n",
        "            \"ide6f51f20-0250-4df5-917b-911cb8a628bf\",\n",
        "            \"id26fe11e5-5525-456d-9e9b-0810e68126df\",\n",
        "            \"idbe59c174-1a15-4682-b335-abff89b5c6db\"\n",
        "        ]\n",
        "    },\n",
        "    'Search': {\n",
        "        'repeat_count': 6,\n",
        "        'task_names': [\n",
        "            \"Libraries_Publishers\",\n",
        "            \"Literature_Quality1\",\n",
        "            \"Literature_Quality2\",\n",
        "            \"Literature_Quality3\",\n",
        "            \"Literature_Quality4\",\n",
        "            \"Overall_Quality_trustworthiness\"\n",
        "        ],\n",
        "        'task_ids': [\n",
        "            \"idb4b827c3-7fa4-4e7c-b8a4-b8697682c5e2\",\n",
        "            \"idf8870479-86b7-423f-b26d-7b0ce10953a0\",\n",
        "            \"id4d2a3973-cd09-4438-9e5e-9cf7018d4856\",\n",
        "            \"idc4302902-36ef-43c1-8461-7fe7003aeae6\",\n",
        "            \"id88dd75cf-1ec0-4d53-bf36-891d0a4ffef8\",\n",
        "            \"idcac92222-7b93-4ddd-ba3f-ad1e102f22a6\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 5: Apply the repeat conditions and assign Task_Name and Task_ID values\n",
        "repeated_rows = []\n",
        "\n",
        "for _, row in df7.iterrows():\n",
        "    # Default values\n",
        "    repeat_count = 1\n",
        "    task_names = [row['Test_Name']]  # Default to the test name itself if no condition matches\n",
        "    task_ids = [None]  # Default to None if no condition matches\n",
        "\n",
        "    # Check for each keyword in the repeat conditions\n",
        "    for key, condition in repeat_conditions.items():\n",
        "        if key in row['Test_Name']:\n",
        "            repeat_count = condition['repeat_count']\n",
        "            task_names = condition['task_names']\n",
        "            task_ids = condition['task_ids']\n",
        "            break  # Exit loop once the condition is matched\n",
        "\n",
        "    # Append each row with its corresponding Task_Name and Task_ID\n",
        "    for i in range(repeat_count):\n",
        "        new_row = row.copy()\n",
        "        new_row['Task_Name'] = task_names[i]\n",
        "        new_row['Task_ID'] = task_ids[i]\n",
        "        repeated_rows.append(new_row)\n",
        "\n",
        "# Create the new DataFrame df7 with repeated rows, Task_Name, and Task_ID\n",
        "df7 = pd.DataFrame(repeated_rows)\n",
        "\n",
        "# Step 6: Reset the index to maintain a clean DataFrame\n",
        "df7 = df7.reset_index(drop=True)\n",
        "\n",
        "# Define the Max_Score mapping for each Task_Name\n",
        "max_score_mapping = {\n",
        "    \"Scientific_Presentation_Style1\": 10,\n",
        "    \"Scientific_Presentation_Style2\": 8,\n",
        "    \"Typical_Question1\": 1,\n",
        "    \"Typical_Question2\": 1,\n",
        "    \"Typical_Question3\": 1,\n",
        "    \"Typical_Question4\": 1,\n",
        "    \"Direct_Quotation1\": 1,\n",
        "    \"Direct_Quotation2\": 1,\n",
        "    \"Indirect_Quotation\": 1,\n",
        "    \"Libraries_Publishers\": 5,\n",
        "    \"Literature_Quality1\": 1,\n",
        "    \"Literature_Quality2\": 1,\n",
        "    \"Literature_Quality3\": 1,\n",
        "    \"Literature_Quality4\": 1,\n",
        "    \"Overall_Quality_trustworthiness\": 3\n",
        "}\n",
        "\n",
        "# Apply the Max_Score mapping to the dataframe\n",
        "df7['Max_Score'] = df7['Task_Name'].map(max_score_mapping)\n",
        "\n",
        "# Function to generate random points\n",
        "def generate_points(max_score):\n",
        "    if max_score == 1:\n",
        "        return np.random.choice([0, 0.5, 1])\n",
        "    else:\n",
        "        return np.random.randint(1, max_score + 1)\n",
        "\n",
        "# Apply function to create 'Points_Reached' column\n",
        "df7['Points_Reached'] = df7['Max_Score'].apply(generate_points)\n",
        "\n",
        "# Create 'Serial' column with sequential numbers from 1 to number of rows\n",
        "df7['Serial'] = range(1, len(df7) + 1)\n",
        "\n",
        "# Reorder columns\n",
        "df7 = df7[['Serial', 'Student_ID', 'Test_ID', 'Test_Name', 'Test_Type', 'Task_Name', 'Task_ID', 'Points_Reached', 'Max_Score']]\n",
        "\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df7)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h9Pd9XUgAGt",
        "outputId": "950ec0db-fcf2-4e6c-ff40-d6de9b4be8d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Serial  Student_ID  Test_ID               Test_Name Test_Type  \\\n",
            "0           1         452        1  Self_Test_Presentation        ST   \n",
            "1           2         452        1  Self_Test_Presentation        ST   \n",
            "2           3         452        3          ARS_Report_Gr1       ARS   \n",
            "3           4         452        3          ARS_Report_Gr1       ARS   \n",
            "4           5         452        3          ARS_Report_Gr1       ARS   \n",
            "...       ...         ...      ...                     ...       ...   \n",
            "15445   15446         282     5053  Self_Test_Presentation        ST   \n",
            "15446   15447         282     5054    Self_Test_Discussion        ST   \n",
            "15447   15448         282     5054    Self_Test_Discussion        ST   \n",
            "15448   15449         282     5054    Self_Test_Discussion        ST   \n",
            "15449   15450         282     5054    Self_Test_Discussion        ST   \n",
            "\n",
            "                            Task_Name                                 Task_ID  \\\n",
            "0      Scientific_Presentation_Style1  idaf224ec3-be2d-45bb-9f3c-acc818407763   \n",
            "1      Scientific_Presentation_Style2  idfce2e221-452f-4828-bd05-266c2d27e3c0   \n",
            "2                   Direct_Quotation1  ide6f51f20-0250-4df5-917b-911cb8a628bf   \n",
            "3                   Direct_Quotation2  id26fe11e5-5525-456d-9e9b-0810e68126df   \n",
            "4                  Indirect_Quotation  idbe59c174-1a15-4682-b335-abff89b5c6db   \n",
            "...                               ...                                     ...   \n",
            "15445  Scientific_Presentation_Style2  idfce2e221-452f-4828-bd05-266c2d27e3c0   \n",
            "15446               Typical_Question1  id072938f8-9d4c-464a-a096-7779cde74fe6   \n",
            "15447               Typical_Question2  id6c761817-f2fe-47bd-8499-540aeed1cbcf   \n",
            "15448               Typical_Question3  id615426a1-9c8d-487a-9a49-d8636a489b31   \n",
            "15449               Typical_Question4  id4ee22677-08a9-4e7d-94d1-a9bf1fed9aaa   \n",
            "\n",
            "       Points_Reached  Max_Score  \n",
            "0                10.0         10  \n",
            "1                 7.0          8  \n",
            "2                 1.0          1  \n",
            "3                 1.0          1  \n",
            "4                 1.0          1  \n",
            "...               ...        ...  \n",
            "15445             4.0          8  \n",
            "15446             1.0          1  \n",
            "15447             0.0          1  \n",
            "15448             0.0          1  \n",
            "15449             1.0          1  \n",
            "\n",
            "[15450 rows x 9 columns]\n"
          ]
        }
      ]
    }
  ]
}